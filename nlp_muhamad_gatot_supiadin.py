# -*- coding: utf-8 -*-
"""NLP-Muhamad Gatot Supiadin

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oheeTK65rYb0Kdbr5Fqe5Kgen1sF0JDV

# **Muhamad Gatot Supiadin**
## M183X0343 | M01 - Pengembangan Machine Learning dan Front End Web
## Universitas Amikom Yogyakarta , Sleman Yogyakarta
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt

df = pd.read_csv('Corona_NLP_train.csv', encoding='latin1')
df.head(10)

df.shape

cols = [4,5]
df = df[df.columns[cols]]
df

first_col = df.pop('Sentiment')
df.insert(0, 'Sentiment', first_col)
df

df.Sentiment.value_counts()

category = pd.get_dummies(df.Sentiment)
df_new = pd.concat([df, category], axis=1)
df_new = df_new.drop(columns=['Sentiment'])
df_new.head(5)

df_new.columns

df2 = df_new['OriginalTweet'].values
label = df_new[['Extremely Negative', 'Extremely Positive',	'Negative',	'Neutral',	'Positive']].values

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(df2, label, test_size=0.2, random_state=1)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout
from tensorflow.keras.models import Sequential

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)

sekuens_latih = tokenizer.texts_to_sequences(X_train)
sekuens_test = tokenizer.texts_to_sequences(X_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)
print(padded_test.shape)

padded_test

padded_latih

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

class CallbackNLP(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy') >= 0.85):
            print("\nReached %2.2f%% accuracy, training has been stop" %(logs.get('accuracy')*100))
            self.model.stop_training = True

callbacks = CallbackNLP()

num_epochs = 50
history = model.fit(padded_latih, y_train, epochs=num_epochs, validation_data=(padded_test, y_test), verbose=2, callbacks=[callbacks])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Accuracy Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()